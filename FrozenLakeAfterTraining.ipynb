{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000  episodes: 0.047\n",
      "2000  episodes: 0.224\n",
      "3000  episodes: 0.412\n",
      "4000  episodes: 0.549\n",
      "5000  episodes: 0.662\n",
      "6000  episodes: 0.712\n",
      "7000  episodes: 0.702\n",
      "8000  episodes: 0.734\n",
      "9000  episodes: 0.699\n",
      "10000  episodes: 0.72\n",
      "Q-table\n",
      " [[0.50565059 0.4694427  0.45826787 0.46488802]\n",
      " [0.3141382  0.25544101 0.32516204 0.50743609]\n",
      " [0.37211395 0.37638461 0.37290648 0.46266395]\n",
      " [0.36366193 0.29754882 0.31866926 0.44205291]\n",
      " [0.52027395 0.40642312 0.30261254 0.46692857]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.16100626 0.11625868 0.19601687 0.16038885]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.35131027 0.40472803 0.2825338  0.54097069]\n",
      " [0.46173941 0.60288191 0.43377237 0.4516411 ]\n",
      " [0.57438378 0.30346852 0.36156682 0.33213606]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.41506527 0.44012419 0.76212413 0.38904119]\n",
      " [0.69105309 0.86056568 0.72598444 0.71275737]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.stats import pearsonr\n",
    "from IPython.display import clear_output\n",
    "\n",
    "env=gym.make(\"FrozenLake-v0\")\n",
    "action_space_size=env.action_space.n\n",
    "state_space_size=env.observation_space.n\n",
    "q_table=np.zeros((state_space_size,action_space_size))\n",
    "#tried to implement the following research paper:-\n",
    "#Reinforcement Learning in Video Games Using NearestNeighborInterpolation and MetricLearning\n",
    "#Matthew S. Emigh, Evan G. Kriminger, Austin J.Brockmeier, Member, IEEE, José C. Príncipe, Fellow, IEEE, and Panos M. Pardalos\n",
    "\n",
    "'''\n",
    "\n",
    "sigma=10\n",
    "random.seed(30)\n",
    "Y=q_table()\n",
    "X=np.random.random(low=0,high=state_space_size-1,size=(state_space_size,action_space_size))\n",
    "w = (2*np.random.rand(state_space_size,action_space_size) - 1)/2\n",
    "\n",
    "for action in action_space_size:\n",
    "    for epochs in range(100):\n",
    "        corr_X_Y=[]\n",
    "    \n",
    "        for j in range(state_space_size):\n",
    "        \n",
    "            kX=np.exp(-w[:,action].T.dot(np.square(X[j][action]-X[:,action])))\n",
    "            kY=np.exp(-(np.square(Y[i][j]-Y[:,j])/(2*sigma*sigma)))\n",
    "            #H=np.identity(state_space_size)-(1/state_space_size)*(np.ones((16,1)).dot(np.ones(16,1).T))\n",
    "        \n",
    "            #X_=np.dot(H.dot(X),H)\n",
    "            #Y_=np.dot(H.dot(Y),H)\n",
    "            for i in range(state_space_size):\n",
    "                corr_X_Y.append(pearsonr(kX[i],kY[i]))\n",
    "        \n",
    "        F=np.log(corr_X_Y)\n",
    "           \n",
    "        \n",
    "            \n",
    "'''\n",
    "episodes=10000\n",
    "max_steps_per_episode=200\n",
    "exploration_rate=1\n",
    "min_exploration_rate=0.001\n",
    "max_exploration_rate=1\n",
    "exploration_decay=0.001\n",
    "\n",
    "learning_rate=0.1\n",
    "discount_rate=0.99\n",
    "rewards_all_episodes=[]\n",
    "for i in range(episodes):\n",
    "    state=env.reset()\n",
    "    reward_per_episode=0\n",
    "    done=False\n",
    "    for steps in range(max_steps_per_episode):\n",
    "        threshold=np.random.uniform(0,1)\n",
    "        if(exploration_rate>threshold):\n",
    "            action=env.action_space.sample()\n",
    "        else:\n",
    "            action=np.argmax(q_table[state,:])\n",
    "\n",
    "        new_state,reward,done,info=env.step(action)\n",
    "        reward_per_episode+=reward\n",
    "        q_table[state,action]=q_table[state,action]*(1-learning_rate)+learning_rate*(reward+discount_rate*np.max(q_table[new_state,:]))\n",
    "        state=new_state\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        #def q_table():\n",
    "            #if episodes==2000:\n",
    "                #return q_table\n",
    "    exploration_rate=min_exploration_rate+(max_exploration_rate-min_exploration_rate)*np.exp(-exploration_decay*i)\n",
    "    rewards_all_episodes.append(reward_per_episode)\n",
    "reward_per_1000_epi=np.split(np.array(rewards_all_episodes),episodes/1000)\n",
    "\n",
    "#after training\n",
    "#lets play\n",
    "Time=[]\n",
    "for episode in range(5):\n",
    "    print(\"******episode \",episode+1,\"*******\")\n",
    "    time.sleep(3)\n",
    "    start=time.time()\n",
    "    state=env.reset()\n",
    "    done=False\n",
    "    for steps in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action=np.argmax(q_table[state,:])\n",
    "        new_state,reward,done,info=env.step(action)\n",
    "        state=new_state\n",
    "        if done:\n",
    "            if reward==1:\n",
    "                print(\"Reached the goal!!!\")\n",
    "                stop=time.time()\n",
    "                print('Time taken to reach the goal:',stop-start, 'seconds')\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"You hit a hole\")\n",
    "                stop=time.time()\n",
    "                print('Hits the hole in ',stop-start, 'seconds')\n",
    "                time.sleep(3)\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            break;\n",
    "#Accuracy\n",
    "count=1000\n",
    "for i in reward_per_1000_epi:\n",
    "    print(count,\" episodes:\",np.mean(i))\n",
    "    count+=1000\n",
    "print('Q-table\\n',q_table)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0b4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
